{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5cf0c2-1fd3-4079-825e-013dd0067d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a4610-9840-40ba-804e-4549b9fd1f36",
   "metadata": {},
   "source": [
    "**Local:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f677fb-6813-4755-a897-704958068b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "import constraints\n",
    "\n",
    "import datagolf_api as dapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15191a0-2600-44ab-8422-57d6ac1da438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=67418968064, available=63281135616, percent=6.1, used=3356487680, free=60972302336, active=1187938304, inactive=3526074368, buffers=250441728, cached=2839736320, shared=38219776, slab=1285459968)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddc7c5-ab55-4d20-8005-19a42bb3026b",
   "metadata": {},
   "source": [
    "**Optimizer: (bottom)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c795bdd0-8eb7-4840-83a1-0eb3c1007929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "/dev/shm is 40B   =   4e-08KiB     \n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "from functools import cache\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(use_memory_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517ec99c-5e0c-4851-ab67-1b5dcd789e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "constants.pandas_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bc5c91-4717-4eb7-9758-b8239bcfdf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_fanduel():\n",
    "    df = pd.read_csv(f'../data/contest-files/{constants.tournament}.csv', usecols=constants.keep_cols)\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    ret = (df\n",
    "           .rename({'nickname': 'name'}, axis=1)\n",
    "           #.loc[(df['injury indicator']!='O') & (df['salary']>7000)]\n",
    "           .drop('injury indicator', axis=1)\n",
    "           .dropna()\n",
    "           .reset_index(drop=True)\n",
    "          )\n",
    "\n",
    "    col_types = { 'name': 'str', 'fppg': 'float' }\n",
    "    \n",
    "    for col in ret.columns:\n",
    "        ret[col] = ret[col].astype(col_types.get(col,'int'))\n",
    "    \n",
    "    ret.to_pickle('../data/pickle-buffer/fanduel-data.pkl')\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_fanduel():\n",
    "    \n",
    "    edit_fanduel()\n",
    "    \n",
    "    return pd.read_pickle('../data/pickle-buffer/fanduel-data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7bb2ef2-7946-4f4e-8299-deb1eb487bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "strokes_gained_components = {\n",
    "    'tee': {\n",
    "        'url_id': 2567,\n",
    "        #'url': 'https://www.pgatour.com/stats/stat.02567.html',\n",
    "        'shortened': 'ott'\n",
    "    },\n",
    "    'approach': {\n",
    "        'url_id': 2568,\n",
    "        #'url': 'https://www.pgatour.com/stats/stat.02568.html',\n",
    "        'shortened': 'app'\n",
    "    },\n",
    "    'around': {\n",
    "        'url_id': 2569,\n",
    "        #'url': 'https://www.pgatour.com/stats/stat.02569.html',\n",
    "        'shortened': 'arg'\n",
    "    },\n",
    "    'green': {\n",
    "        'url_id': 2564,\n",
    "        #'url': 'https://www.pgatour.com/stats/stat.02564.html',\n",
    "        'shortened': 'putt'\n",
    "    },\n",
    "    'tee-to-green': {\n",
    "        'url_id': 2674,\n",
    "        #'url': 'https://www.pgatour.com/stats/stat.02674.html',\n",
    "        'shortened' : 'ttg'\n",
    "        }\n",
    "}\n",
    "\n",
    "new_col_names = {\n",
    "    'player name': 'name',\n",
    "    'rank this week': ' cur-rank',\n",
    "    'rank last week': ' prev-rank',\n",
    "    'average': ' sg',\n",
    "    'rounds': ' num-rounds',\n",
    "    'measured rounds': ' num-measured'\n",
    "}\n",
    "\n",
    "abbrev_col_names = [ 'name', ' sg' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779dfaa6-f14c-4a83-9235-82c2d4a353fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strokes_gained_per(golf_shot, abbreviate=True):\n",
    "    \n",
    "    if golf_shot.lower() not in strokes_gained_components:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        info = strokes_gained_components.get(golf_shot.lower(), None)\n",
    "        if info is None:\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            url = f'https://www.pgatour.com/stats/stat.0{ info[\"url_id\"] }.html'\n",
    "            \n",
    "            ret = pd.read_html(url)[1].reset_index(drop=True)\n",
    "            \n",
    "            ret.columns = ret.columns.str.lower().str.replace('total sg:', ' sg').str.replace('\\xa0', ' ')\n",
    "            \n",
    "            ret = ret.rename(new_col_names, axis=1)\n",
    "            #ret.index = ret['name']\n",
    "            #ret = ret.drop('name', axis=1)\n",
    "            if abbreviate:\n",
    "                ret = ret.loc[:, abbrev_col_names]\n",
    "            \n",
    "            ret.columns = ret.columns.str.replace(' ', f'{strokes_gained_components[golf_shot][\"shortened\"]}-')\n",
    "            \n",
    "            ret.to_pickle(f'../data/pickle-buffer/{strokes_gained_components[golf_shot][\"shortened\"]}-sg.pkl')\n",
    "            \n",
    "            return None\n",
    "        \n",
    "def load_strokes_gained_per(golf_shot):\n",
    "    \n",
    "    #**\n",
    "    strokes_gained_per(golf_shot)\n",
    "    \n",
    "    return pd.read_pickle(f'../data/pickle-buffer/{strokes_gained_components[golf_shot][\"shortened\"]}-sg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c837c8f-84de-4731-9389-3b4630d714d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_strokes_gained():\n",
    "    \n",
    "    # Create dictionary containing strokes-gained data for each stroke\n",
    "    sg_frames = { golf_shot: load_strokes_gained_per(golf_shot) for golf_shot in strokes_gained_components }\n",
    "\n",
    "    # Initialize frame as tee and merge rest of shots\n",
    "    sgdf = sg_frames['tee']\n",
    "    for k in list(strokes_gained_components.keys())[1:]:\n",
    "        sgdf = sgdf.merge(sg_frames[k])\n",
    "    \n",
    "    ret = (sgdf\n",
    "           #.sort_values(by=constants.focus_stat, ascending=False)\n",
    "           .reset_index(drop=True)\n",
    "          )\n",
    "\n",
    "    ret.to_pickle(f'../data/pickle-buffer/strokes-gained.pkl')                                                 \n",
    "                                                 \n",
    "    return None\n",
    "\n",
    "def load_strokes_gained():\n",
    "    \n",
    "    aggregate_strokes_gained()\n",
    "    \n",
    "    return pd.read_pickle('../data/pickle-buffer/strokes-gained.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b085b7-b0ec-41fa-93b8-05b9fe76fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pga_fanduel():\n",
    "    fd = load_fanduel()\n",
    "    sg = load_strokes_gained()\n",
    "    \n",
    "    focus_stats = [ constants.focus_stat ]\n",
    "    if constants.focus_stat_2 is not None:\n",
    "        focus_stats.append(constants.focus_stat_2)\n",
    "        if constants.focus_stat_3 is not None:\n",
    "            focus_stats.append(constants.focus_stat_3)\n",
    "    \n",
    "    focus_stats = tuple(focus_stats)\n",
    "    \n",
    "    sg_lookup = load_strokes_gained()\n",
    "    sg_lookup.index = sg_lookup['name']\n",
    "    sg_lookup = sg_lookup.drop('name', axis=1)\n",
    "    \n",
    "    for sg_col in focus_stats:\n",
    "        fd[sg_col] = fd['name'].apply(lambda x: sg_lookup.loc[x, sg_col] if x in sg_lookup.index else 0.0)\n",
    "        fd[f'{sg_col}-per-10k'] = np.array( 10000 * fd[sg_col] / fd['salary'] )\n",
    "    \n",
    "    #for sg_col in focus_stats:\n",
    "        #fd[sg_col] = fd['name'].apply(lambda x: sg.loc[x, sg_col] if x in sg.index else 0.0)\n",
    "        #fd[f'{sg_col}-per-10k'] = np.array( 10000 * fd[sg_col] / fd['salary'] )\n",
    "    \n",
    "    #convs = {'name': 'str', 'salary': 'int'}\n",
    "    \n",
    "    #for col in fd.columns:\n",
    "        #fd[col] = fd[col].astype(convs.get(col, 'float'))\n",
    "    \n",
    "    # fd.index = fd['name']\n",
    "    # fd = fd.drop('name', axis=1)\n",
    "                           \n",
    "    fd = (fd\n",
    "          .sort_values(by=[constants.focus_stat], ascending=False)\n",
    "          .dropna()\n",
    "         )\n",
    "    \n",
    "    fd.to_pickle(f'../data/pickle-buffer/{constants.tournament}.pkl')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b80cdff9-b699-45f3-946a-ca7919b3459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_constraints():\n",
    "    \n",
    "    combine_pga_fanduel()\n",
    "    ret = pd.read_pickle(f'../data/pickle-buffer/{constants.tournament}.pkl')\n",
    "    \n",
    "    if constraints.min_salary is not None:\n",
    "        \n",
    "        ret = (ret\n",
    "               .loc[ ret['salary']>=constraints.min_salary ]\n",
    "               .reset_index(drop=True)\n",
    "              )\n",
    "\n",
    "        \n",
    "#    ret = (ret\n",
    "#           .loc[ ret['name'].isin(dapi.players_who_made_cut()) ]\n",
    "#           .reset_index(drop=True)\n",
    "#          )\n",
    "    \n",
    "    ret.to_pickle(f'../data/pickle-buffer/optimizer-data.pkl')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54aea71a-b090-4ca5-ac86-df4bc3fdcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input():\n",
    "    \n",
    "    add_constraints()\n",
    "    ret = pd.read_pickle(f'../data/pickle-buffer/optimizer-data.pkl')\n",
    "    ret_names = ret['name'].values.tolist()\n",
    "    \n",
    "    ret['proj-pts'] = ret['name'].apply(dapi.proj_pts)\n",
    "    ret['cfit-adj'] = ret['name'].apply(dapi.proj_skd)\n",
    "    \n",
    "    ret['cfit-pts'] = ret['proj-pts']+(ret['proj-pts']*ret['cfit-adj'])\n",
    "    \n",
    "    print(ret.sort_values(by='cfit-pts', ascending=False).head(7))\n",
    "    \n",
    "    ret['salary'] /= 100\n",
    "    ret.index = ret['name']\n",
    "    ret = ret.drop('name', axis=1)\n",
    "    \n",
    "    ret.to_pickle(f'../data/pickle-buffer/optimizer-data-clean.pkl')\n",
    "    \n",
    "    return ret_names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1bcc7-fc63-4a41-987b-4f62d033881c",
   "metadata": {},
   "source": [
    "`from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def Multi(a, b):\n",
    "    return a*b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45aa0011-d3be-40af-a9f8-80c702fa4f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name  fppg  played  salary  putt-sg  putt-sg-per-10k  ott-sg  ott-sg-per-10k  arg-sg  arg-sg-per-10k  proj-pts  cfit-adj  cfit-pts\n",
      "17        Sungjae Im 72.96      19   11500     0.16             0.14    0.58            0.50    0.51            0.45     95.02      1.41    229.06\n",
      "50        Tony Finau 68.42      20   12000    -0.15            -0.12    0.49            0.41    0.17            0.14     90.60      1.22    200.99\n",
      "40  Hideki Matsuyama 79.11      18   11900    -0.05            -0.05    0.21            0.17    0.20            0.16     90.49      1.20    199.35\n",
      "15  Cameron Tringale 65.04      23   11000     0.19             0.17   -0.11           -0.10    0.12            0.11     82.85      0.86    154.22\n",
      "72    Brendan Steele 55.83      18   10000    -0.47            -0.47    0.78            0.78   -0.12           -0.12     82.24      0.80    148.17\n",
      "8   Maverick McNealy 73.67      21   11300     0.40             0.35    0.30            0.26   -0.01           -0.01     82.43      0.80    148.00\n",
      "16       Adam Hadwin 66.28      21   11400     0.17             0.15   -0.06           -0.05    0.49            0.43     81.31      0.80    146.09\n"
     ]
    }
   ],
   "source": [
    "pnames = prepare_input()\n",
    "data = pd.read_pickle(f'../data/pickle-buffer/optimizer-data-clean.pkl')\n",
    "\n",
    "@cache\n",
    "def get_value(name, column):\n",
    "    return( data.loc[name, column] )\n",
    "\n",
    "@cache\n",
    "def sum_values(names, column):\n",
    "    return( sum( [ get_value(name, column) for name in names ] ) )\n",
    "\n",
    "@cache\n",
    "def is_valid_lineup(lineup):\n",
    "    return( sum_values(lineup, 'salary') in constraints.cost_range and len(set(lineup))==6 )\n",
    "\n",
    "@cache\n",
    "def lineup_analysis(lineup):\n",
    "    return(tuple( [ sum_values(tuple(set(lineup)),column) for column in constraints.cols_to_sum ] ) )\n",
    "\n",
    "def lineup_analysis_wrapper(lineup):\n",
    "    return( lineup_analysis(tuple(set(lineup.to_numpy()))) if is_valid_lineup(tuple(set(lineup.to_numpy()))) else (0.0,)*len(constraints.cols_to_sum)  )\n",
    "\n",
    "def create_lineup_2_slices(slate_dict):\n",
    "#     2 things of three\n",
    "    ret_list = list()\n",
    "    \n",
    "    for half_slates in tqdm( [p for p in itertools.product(*slate_dict.values())] ):\n",
    "        \n",
    "        g1,g2,g3 = tuple(sorted(list(half_slates[0])))\n",
    "        g4,g5,g6 = tuple(sorted(list(half_slates[1])))\n",
    "        \n",
    "        lu = (g1,g2,g3,g4,g5,g6)\n",
    "        if is_valid_lineup(lu):\n",
    "            ret_list.append(lu)\n",
    "    \n",
    "    return(tuple(ret_list))\n",
    "\n",
    "def create_lineup_3_slices(slate_dict):\n",
    "    \n",
    "#     3 things of two\n",
    "    return None\n",
    "\n",
    "# Trying to get better about only passing tuples or other completely immutable for default and for cache\n",
    "def create_lineups():\n",
    "    \n",
    "    # Not necessary but makes reading easier\n",
    "    num_players = 6 # (n)\n",
    "    num_slices = constraints.slices\n",
    "    step = int( len(pnames) / num_slices )\n",
    "    \n",
    "    r = int(num_players / num_slices) # (nCr)\n",
    "    \n",
    "\n",
    "    # slates = {f'slate{i+1}': tuple(map( tuple, itertools.combinations(pnames[:int(i*step)], r) )) for i in range(num_slices+1)}\n",
    "    # slates.update( { f'slate{num_slices}': tuple(map( tuple, itertools.combinations(pnames[int(num_slices*step):], r) )) } )\n",
    "    \n",
    "    slates = dict()\n",
    "    \n",
    "    if num_slices == 2:\n",
    "        \n",
    "        slates = {\n",
    "            'slate1': tuple(map( tuple, itertools.combinations(pnames[:step], r) )),\n",
    "            'slate2': tuple(map( tuple, itertools.combinations(pnames[step:], r) ))\n",
    "        }\n",
    "        \n",
    "    elif num_slices == 3:\n",
    "        \n",
    "        slates = {\n",
    "            'slate1': tuple(map( tuple, itertools.combinations(pnames[:step], r) )),\n",
    "            'slate2': tuple(map( tuple, itertools.combinations(pnames[step:int(2*step)], r) )),\n",
    "            'slate3': tuple(map( tuple, itertools.combinations(pnames[int(2*step):], r) )),\n",
    "            \n",
    "        }\n",
    "    \n",
    "    operations = { 2: create_lineup_2_slices(slates), 3: create_lineup_3_slices(slates)}\n",
    "    \n",
    "    lineups = operations[num_slices]\n",
    "    \n",
    "    ret = pd.DataFrame(lineups, columns=['g1','g2','g3','g4','g5','g6'])\n",
    "    #ret[constraints.cols_to_sum] = ret.apply( lineup_analysis_wrapper, axis=1, result_type='expand')\n",
    "    ret[constraints.cols_to_sum] = ret.parallel_apply( lineup_analysis_wrapper, axis=1, result_type='expand' )\n",
    "    \n",
    "    ret = (ret\n",
    "           # .sort_values(by=f'{constants.focus_stat}-per-10k', ascending=False)\n",
    "           .sort_values(by='proj-pts', ascending=False)\n",
    "           .drop_duplicates()\n",
    "           .reset_index(drop=True)\n",
    "          )\n",
    "    \n",
    "    \n",
    "    ret.to_pickle(f'../data/lineups-created/{constants.tournament}.pkl')\n",
    "    \n",
    "    print('Done...')\n",
    "    \n",
    "    return None\n",
    "\n",
    "def output_lineups(top_num=100):\n",
    "    return pd.read_pickle(f'../data/lineups-created/{constants.tournament}.pkl').head(top_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e61588f-de3e-436c-b1bc-3c55bee3409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_lineups_by(sort_by=('proj-pts',)):\n",
    "    return pd.read_pickle(f'../data/lineups-created/{constants.tournament}.pkl').sort_values(by=sort_by[0], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca102ee-44c6-451b-9045-9288d65fa282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_func():\n",
    "    if constants.create:\n",
    "        create_lineups()\n",
    "    return output_lineups_by()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7923b-7cc5-4e68-b61c-068baa6ad623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d269bba13f45aa97b7800cd570c56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71166096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef48292-3f98-4474-b35a-6dd4833a9b4c",
   "metadata": {},
   "source": [
    "#### *Logs for different /dev/shm sizes*\n",
    "\n",
    "| --- | --- | --- | --- |\n",
    "| Size | Time | Constraints | it/s |\n",
    "| --- | --- | --- | --- | \n",
    "| 8GB |  | min_salary=$8K,min_cost=$59.5K| 200K |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
